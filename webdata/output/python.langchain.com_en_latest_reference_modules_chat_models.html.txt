.rst
.pdf
Chat Models
Chat Models
#
pydantic
model
langchain.chat_models.
AzureChatOpenAI
[source]
#
Wrapper around Azure OpenAI Chat Completion API. To use this class you
must have a deployed model on Azure OpenAI. Use
deployment_name
in the
constructor to refer to the “Model deployment name” in the Azure portal.
In addition, you should have the
openai
python package installed, and the
following environment variables set or passed in constructor in lower case:
-
OPENAI_API_TYPE
(default:
azure
)
-
OPENAI_API_KEY
-
OPENAI_API_BASE
-
OPENAI_API_VERSION
-
OPENAI_PROXY
For exmaple, if you have
gpt-35-turbo
deployed, with the deployment name
35-turbo-dev
, the constructor should look like:
AzureChatOpenAI
(
deployment_name
=
"35-turbo-dev"
,
openai_api_version
=
"2023-03-15-preview"
,
)
Be aware the API version may change.
Any parameters that are valid to be passed to the openai.create call can be passed
in, even if not explicitly saved on this class.
field
deployment_name
:
str
=
''
#
field
openai_api_base
:
str
=
''
#
field
openai_api_key
:
str
=
''
#
Base URL path for API requests,
leave blank if not using a proxy or service emulator.
field
openai_api_type
:
str
=
'azure'
#
field
openai_api_version
:
str
=
''
#
field
openai_organization
:
str
=
''
#
field
openai_proxy
:
str
=
''
#
pydantic
model
langchain.chat_models.
ChatAnthropic
[source]
#
Wrapper around Anthropic’s large language model.
To use, you should have the
anthropic
python package installed, and the
environment variable
ANTHROPIC_API_KEY
set with your API key, or pass
it as a named parameter to the constructor.
Example
import
anthropic
from
langchain.llms
import
Anthropic
model
=
ChatAnthropic
(
model
=
"<model_name>"
,
anthropic_api_key
=
"my-api-key"
)
get_num_tokens
(
text
:
str
)
→
int
[source]
#
Calculate number of tokens.
pydantic
model
langchain.chat_models.
ChatGooglePalm
[source]
#
Wrapper around Google’s PaLM Chat API.
To use you must have the google.generativeai Python package installed and
either:
The
GOOGLE_API_KEY`
environment varaible set with your API key, or
Pass your API key using the google_api_key kwarg to the ChatGoogle
constructor.
Example
from
langchain.chat_models
import
ChatGooglePalm
chat
=
ChatGooglePalm
()
field
google_api_key
:
Optional
[
str
]
=
None
#
field
model_name
:
str
=
'models/chat-bison-001'
#
Model name to use.
field
n
:
int
=
1
#
Number of chat completions to generate for each prompt. Note that the API may
not return the full n completions if duplicates are generated.
field
temperature
:
Optional
[
float
]
=
None
#
Run inference with this temperature. Must by in the closed
interval [0.0, 1.0].
field
top_k
:
Optional
[
int
]
=
None
#
Decode using top-k sampling: consider the set of top_k most probable tokens.
Must be positive.
field
top_p
:
Optional
[
float
]
=
None
#
Decode using nucleus sampling: consider the smallest set of tokens whose
probability sum is at least top_p. Must be in the closed interval [0.0, 1.0].
pydantic
model
langchain.chat_models.
ChatOpenAI
[source]
#
Wrapper around OpenAI Chat large language models.
To use, you should have the
openai
python package installed, and the
environment variable
OPENAI_API_KEY
set with your API key.
Any parameters that are valid to be passed to the openai.create call can be passed
in, even if not explicitly saved on this class.
Example
from
langchain.chat_models
import
ChatOpenAI
openai
=
ChatOpenAI
(
model_name
=
"gpt-3.5-turbo"
)
field
max_retries
:
int
=
6
#
Maximum number of retries to make when generating.
field
max_tokens
:
Optional
[
int
]
=
None
#
Maximum number of tokens to generate.
field
model_kwargs
:
Dict
[
str
,
Any
]
[Optional]
#
Holds any model parameters valid for
create
call not explicitly specified.
field
model_name
:
str
=
'gpt-3.5-turbo'
(alias
'model')
#
Model name to use.
field
n
:
int
=
1
#
Number of chat completions to generate for each prompt.
field
openai_api_base
:
Optional
[
str
]
=
None
#
field
openai_api_key
:
Optional
[
str
]
=
None
#
Base URL path for API requests,
leave blank if not using a proxy or service emulator.
field
openai_organization
:
Optional
[
str
]
=
None
#
field
openai_proxy
:
Optional
[
str
]
=
None
#
field
request_timeout
:
Optional
[
Union
[
float
,
Tuple
[
float
,
float
]
]
]
=
None
#
Timeout for requests to OpenAI completion API. Default is 600 seconds.
field
streaming
:
bool
=
False
#
Whether to stream the results or not.
field
temperature
:
float
=
0.7
#
What sampling temperature to use.
completion_with_retry
(
**
kwargs
:
Any
)
→
Any
[source]
#
Use tenacity to retry the completion call.
get_num_tokens_from_messages
(
messages
:
List
[
langchain.schema.BaseMessage
]
)
→
int
[source]
#
Calculate num tokens for gpt-3.5-turbo and gpt-4 with tiktoken package.
Official documentation:
openai/openai-cookbook
main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb
get_token_ids
(
text
:
str
)
→
List
[
int
]
[source]
#
Get the tokens present in the text with tiktoken package.
pydantic
model
langchain.chat_models.
ChatVertexAI
[source]
#
Wrapper around Vertex AI large language models.
field
model_name
:
str
=
'chat-bison'
#
Model name to use.
pydantic
model
langchain.chat_models.
PromptLayerChatOpenAI
[source]
#
Wrapper around OpenAI Chat large language models and PromptLayer.
To use, you should have the
openai
and
promptlayer
python
package installed, and the environment variable
OPENAI_API_KEY
and
PROMPTLAYER_API_KEY
set with your openAI API key and
promptlayer key respectively.
All parameters that can be passed to the OpenAI LLM can also
be passed here. The PromptLayerChatOpenAI adds to optional
Parameters
pl_tags
– List of strings to tag the request with.
return_pl_id
– If True, the PromptLayer request ID will be
returned in the
generation_info
field of the
Generation
object.
Example
from
langchain.chat_models
import
PromptLayerChatOpenAI
openai
=
PromptLayerChatOpenAI
(
model_name
=
"gpt-3.5-turbo"
)
field
pl_tags
:
Optional
[
List
[
str
]
]
=
None
#
field
return_pl_id
:
Optional
[
bool
]
=
False
#
previous
Models
next
Embeddings
By Harrison Chase
© Copyright 2023, Harrison Chase.
Last updated on Jun 09, 2023.
.rst
.pdf
Embeddings
Embeddings
#
Wrappers around embedding modules.
pydantic
model
langchain.embeddings.
AlephAlphaAsymmetricSemanticEmbedding
[source]
#
Wrapper for Aleph Alpha’s Asymmetric Embeddings
AA provides you with an endpoint to embed a document and a query.
The models were optimized to make the embeddings of documents and
the query for a document as similar as possible.
To learn more, check out:
https://docs.aleph-alpha.com/docs/tasks/semantic_embed/
Example
from
aleph_alpha
import
AlephAlphaAsymmetricSemanticEmbedding
embeddings
=
AlephAlphaSymmetricSemanticEmbedding
()
document
=
"This is a content of the document"
query
=
"What is the content of the document?"
doc_result
=
embeddings
.
embed_documents
([
document
])
query_result
=
embeddings
.
embed_query
(
query
)
field
aleph_alpha_api_key
:
Optional
[
str
]
=
None
#
API key for Aleph Alpha API.
field
compress_to_size
:
Optional
[
int
]
=
128
#
Should the returned embeddings come back as an original 5120-dim vector,
or should it be compressed to 128-dim.
field
contextual_control_threshold
:
Optional
[
int
]
=
None
#
Attention control parameters only apply to those tokens that have
explicitly been set in the request.
field
control_log_additive
:
Optional
[
bool
]
=
True
#
Apply controls on prompt items by adding the log(control_factor)
to attention scores.
field
hosting
:
Optional
[
str
]
=
'https://api.aleph-alpha.com'
#
Optional parameter that specifies which datacenters may process the request.
field
model
:
Optional
[
str
]
=
'luminous-base'
#
Model name to use.
field
normalize
:
Optional
[
bool
]
=
True
#
Should returned embeddings be normalized
embed_documents
(
texts
:
List
[
str
]
)
→
List
[
List
[
float
]
]
[source]
#
Call out to Aleph Alpha’s asymmetric Document endpoint.
Parameters
texts
– The list of texts to embed.
Returns
List of embeddings, one for each text.
embed_query
(
text
:
str
)
→
List
[
float
]
[source]
#
Call out to Aleph Alpha’s asymmetric, query embedding endpoint
:param text: The text to embed.
Returns
Embeddings for the text.
pydantic
model
langchain.embeddings.
AlephAlphaSymmetricSemanticEmbedding
[source]
#
The symmetric version of the Aleph Alpha’s semantic embeddings.
The main difference is that here, both the documents and
queries are embedded with a SemanticRepresentation.Symmetric
.. rubric:: Example
from
aleph_alpha
import
AlephAlphaSymmetricSemanticEmbedding
embeddings
=
AlephAlphaAsymmetricSemanticEmbedding
()
text
=
"This is a test text"
doc_result
=
embeddings
.
embed_documents
([
text
])
query_result
=
embeddings
.
embed_query
(
text
)
embed_documents
(
texts
:
List
[
str
]
)
→
List
[
List
[
float
]
]
[source]
#
Call out to Aleph Alpha’s Document endpoint.
Parameters
texts
– The list of texts to embed.
Returns
List of embeddings, one for each text.
embed_query
(
text
:
str
)
→
List
[
float
]
[source]
#
Call out to Aleph Alpha’s asymmetric, query embedding endpoint
:param text: The text to embed.
Returns
Embeddings for the text.
pydantic
model
langchain.embeddings.
BedrockEmbeddings
[source]
#
Embeddings provider to invoke Bedrock embedding models.
To authenticate, the AWS client uses the following methods to
automatically load credentials:
https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html
If a specific credential profile should be used, you must pass
the name of the profile from the ~/.aws/credentials file that is to be used.
Make sure the credentials / roles used have the required policies to
access the Bedrock service.
field
credentials_profile_name
:
Optional
[
str
]
=
None
#
The name of the profile in the ~/.aws/credentials or ~/.aws/config files, which
has either access keys or role information specified.
If not specified, the default credential profile or, if on an EC2 instance,
credentials from IMDS will be used.
See:
https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html
field
model_id
:
str
=
'amazon.titan-e1t-medium'
#
Id of the model to call, e.g., amazon.titan-e1t-medium, this is
equivalent to the modelId property in the list-foundation-models api
field
model_kwargs
:
Optional
[
Dict
]
=
None
#
Key word arguments to pass to the model.
field
region_name
:
Optional
[
str
]
=
None
#
The aws region e.g.,
us-west-2
. Fallsback to AWS_DEFAULT_REGION env variable
or region specified in ~/.aws/config in case it is not provided here.
embed_documents
(
texts
:
List
[
str
]
,
chunk_size
:
int
=
1
)
→
List
[
List
[
float
]
]
[source]
#
Compute doc embeddings using a Bedrock model.
Parameters
texts
– The list of texts to embed.
chunk_size
– Bedrock currently only allows single string
inputs, so chunk size is always 1. This input is here
only for compatibility with the embeddings interface.
Returns
List of embeddings, one for each text.
embed_query
(
text
:
str
)
→
List
[
float
]
[source]
#
Compute query embeddings using a Bedrock model.
Parameters
text
– The text to embed.
Returns
Embeddings for the text.
pydantic
model
langchain.embeddings.
CohereEmbeddings
[source]
#
Wrapper around Cohere embedding models.
To use, you should have the
cohere
python package installed, and the
environment variable
COHERE_API_KEY
set with your API key or pass it
as a named parameter to the constructor.
Example
from
langchain.embeddings
import
CohereEmbeddings
cohere
=
CohereEmbeddings
(
model
=
"embed-english-light-v2.0"
,
cohere_api_key
=
"my-api-key"
)
field
model
:
str
=
'embed-english-v2.0'
#
Model name to use.
field
truncate
:
Optional
[
str
]
=
None
#
Truncate embeddings that are too long from start or end (“NONE”|”START”|”END”)
embed_documents
(
texts
:
List
[
str
]
)
→
List
[
List
[
float
]
]
[source]
#
Call out to Cohere’s embedding endpoint.
Parameters
texts
– The list of texts to embed.
Returns
List of embeddings, one for each text.
embed_query
(
text
:
str
)
→
List
[
float
]
[source]
#
Call out to Cohere’s embedding endpoint.
Parameters
text
– The text to embed.
Returns
Embeddings for the text.
pydantic
model
langchain.embeddings.
DeepInfraEmbeddings
[source]
#
Wrapper around Deep Infra’s embedding inference service.
To use, you should have the
environment variable
DEEPINFRA_API_TOKEN
set with your API token, or pass
it as a named parameter to the constructor.
There are multiple embeddings models available,
see
https://deepinfra.com/models?type=embeddings
.
Example
from
langchain.embeddings
import
DeepInfraEmbeddings
deepinfra_emb
=
DeepInfraEmbeddings
(
model_id
=
"sentence-transformers/clip-ViT-B-32"
,
deepinfra_api_token
=
"my-api-key"
)
r1
=
deepinfra_emb
.
embed_documents
(
[
"Alpha is the first letter of Greek alphabet"
,
"Beta is the second letter of Greek alphabet"
,
]
)
r2
=
deepinfra_emb
.
embed_query
(
"What is the second letter of Greek alphabet"
)
field
embed_instruction
:
str
=
'passage:
'
#
Instruction used to embed documents.
field
model_id
:
str
=
'sentence-transformers/clip-ViT-B-32'
#
Embeddings model to use.
field
model_kwargs
:
Optional
[
dict
]
=
None
#
Other model keyword args
field
normalize
:
bool
=
False
#
whether to normalize the computed embeddings
field
query_instruction
:
str
=
'query:
'
#
Instruction used to embed the query.
embed_documents
(
texts
:
List
[
str
]
)
→
List
[
List
[
float
]
]
[source]
#
Embed documents using a Deep Infra deployed embedding model.
Parameters
texts
– The list of texts to embed.
Returns
List of embeddings, one for each text.
embed_query
(
text
:
str
)
→
List
[
float
]
[source]
#
Embed a query using a Deep Infra deployed embedding model.
Parameters
text
– The text to embed.
Returns
Embeddings for the text.
class
langchain.embeddings.
ElasticsearchEmbeddings
(
client
:
MlClient
,
model_id
:
str
,
*
,
input_field
:
str
=
'text_field'
)
[source]
#
Wrapper around Elasticsearch embedding models.
This class provides an interface to generate embeddings using a model deployed
in an Elasticsearch cluster. It requires an Elasticsearch connection object
and the model_id of the model deployed in the cluster.
In Elasticsearch you need to have an embedding model loaded and deployed.
-
https://www.elastic.co/guide/en/elasticsearch/reference/current/infer-trained-model.html
-
https://www.elastic.co/guide/en/machine-learning/current/ml-nlp-deploy-models.html
embed_documents
(
texts
:
List
[
str
]
)
→
List
[
List
[
float
]
]
[source]
#
Generate embeddings for a list of documents.
Parameters
texts
(
List
[
str
]
) – A list of document text strings to generate embeddings
for.
Returns
A list of embeddings, one for each document in the input
list.
Return type
List[List[float]]
embed_query
(
text
:
str
)
→
List
[
float
]
[source]
#
Generate an embedding for a single query text.
Parameters
text
(
str
) – The query text to generate an embedding for.
Returns
The embedding for the input query text.
Return type
List[float]
classmethod
from_credentials
(
model_id
:
str
,
*
,
es_cloud_id
:
Optional
[
str
]
=
None
,
es_user
:
Optional
[
str
]
=
None
,
es_password
:
Optional
[
str
]
=
None
,
input_field
:
str
=
'text_field'
)
→
langchain.embeddings.elasticsearch.ElasticsearchEmbeddings
[source]
#
Instantiate embeddings from Elasticsearch credentials.
Parameters
model_id
(
str
) – The model_id of the model deployed in the Elasticsearch
cluster.
input_field
(
str
) – The name of the key for the input text field in the
document. Defaults to ‘text_field’.
es_cloud_id
– (str, optional): The Elasticsearch cloud ID to connect to.
es_user
– (str, optional): Elasticsearch username.
es_password
– (str, optional): Elasticsearch password.
Example
from
langchain.embeddings
import
ElasticsearchEmbeddings
# Define the model ID and input field name (if different from default)
model_id
=
"your_model_id"
# Optional, only if different from 'text_field'
input_field
=
"your_input_field"
# Credentials can be passed in two ways. Either set the env vars
# ES_CLOUD_ID, ES_USER, ES_PASSWORD and they will be automatically
# pulled in, or pass them in directly as kwargs.
embeddings
=
ElasticsearchEmbeddings
.
from_credentials
(
model_id
,
input_field
=
input_field
,
# es_cloud_id="foo",
# es_user="bar",
# es_password="baz",
)
documents
=
[
"This is an example document."
,
"Another example document to generate embeddings for."
,
]
embeddings_generator
.
embed_documents
(
documents
)
classmethod
from_es_connection
(
model_id
:
str
,
es_connection
:
Elasticsearch
,
input_field
:
str
=
'text_field'
)
→
ElasticsearchEmbeddings
[source]
#
Instantiate embeddings from an existing Elasticsearch connection.
This method provides a way to create an instance of the ElasticsearchEmbeddings
class using an existing Elasticsearch connection. The connection object is used
to create an MlClient, which is then used to initialize the
ElasticsearchEmbeddings instance.
Args:
model_id (str): The model_id of the model deployed in the Elasticsearch cluster.
es_connection (elasticsearch.Elasticsearch): An existing Elasticsearch
connection object. input_field (str, optional): The name of the key for the
input text field in the document. Defaults to ‘text_field’.
Returns:
ElasticsearchEmbeddings: An instance of the ElasticsearchEmbeddings class.
Example
from
elasticsearch
import
Elasticsearch
from
langchain.embeddings
import
ElasticsearchEmbeddings
# Define the model ID and input field name (if different from default)
model_id
=
"your_model_id"
# Optional, only if different from 'text_field'
input_field
=
"your_input_field"
# Create Elasticsearch connection
es_connection
=
Elasticsearch
(
hosts
=
[
"localhost:9200"
],
http_auth
=
(
"user"
,
"password"
)
)
# Instantiate ElasticsearchEmbeddings using the existing connection
embeddings
=
ElasticsearchEmbeddings
.
from_es_connection
(
model_id
,
es_connection
,
input_field
=
input_field
,
)
documents
=
[
"This is an example document."
,
"Another example document to generate embeddings for."
,
]
embeddings_generator
.
embed_documents
(
documents
)
pydantic
model
langchain.embeddings.
FakeEmbeddings
[source]
#
embed_documents
(
texts
:
List
[
str
]
)
→
List
[
List
[
float
]
]
[source]
#
Embed search docs.
embed_query
(
text
:
str
)
→
List
[
float
]
[source]
#
Embed query text.
pydantic
model
langchain.embeddings.
HuggingFaceEmbeddings
[source]
#
Wrapper around sentence_transformers embedding models.
To use, you should have the
sentence_transformers
python package installed.
Example
from
langchain.embeddings
import
HuggingFaceEmbeddings
model_name
=
"sentence-transformers/all-mpnet-base-v2"
model_kwargs
=
{
'device'
:
'cpu'
}
encode_kwargs
=
{
'normalize_embeddings'
:
False
}
hf
=
HuggingFaceEmbeddings
(
model_name
=
model_name
,
model_kwargs
=
model_kwargs
,
encode_kwargs
=
encode_kwargs
)
field
cache_folder
:
Optional
[
str
]
=
None
#
Path to store models.
Can be also set by SENTENCE_TRANSFORMERS_HOME environment variable.
field
encode_kwargs
:
Dict
[
str
,
Any
]
[Optional]
#
Key word arguments to pass when calling the
encode
method of the model.
field
model_kwargs
:
Dict
[
str
,
Any
]
[Optional]
#
Key word arguments to pass to the model.
field
model_name
:
str
=
'sentence-transformers/all-mpnet-base-v2'
#
Model name to use.
embed_documents
(
texts
:
List
[
str
]
)
→
List
[
List
[
float
]
]
[source]
#
Compute doc embeddings using a HuggingFace transformer model.
Parameters
texts
– The list of texts to embed.
Returns
List of embeddings, one for each text.
embed_query
(
text
:
str
)
→
List
[
float
]
[source]
#
Compute query embeddings using a HuggingFace transformer model.
Parameters
text
– The text to embed.
Returns
Embeddings for the text.
pydantic
model
langchain.embeddings.
HuggingFaceHubEmbeddings
[source]
#
Wrapper around HuggingFaceHub embedding models.
To use, you should have the
huggingface_hub
python package installed, and the
environment variable
HUGGINGFACEHUB_API_TOKEN
set with your API token, or pass
it as a named parameter to the constructor.
Example
from
langchain.embeddings
import
HuggingFaceHubEmbeddings
repo_id
=
"sentence-transformers/all-mpnet-base-v2"
hf
=
HuggingFaceHubEmbeddings
(
repo_id
=
repo_id
,
task
=
"feature-extraction"
,
huggingfacehub_api_token
=
"my-api-key"
,
)
field
model_kwargs
:
Optional
[
dict
]
=
None
#
Key word arguments to pass to the model.
field
repo_id
:
str
=
'sentence-transformers/all-mpnet-base-v2'
#
Model name to use.
field
task
:
Optional
[
str
]
=
'feature-extraction'
#
Task to call the model with.
embed_documents
(
texts
:
List
[
str
]
)
→
List
[
List
[
float
]
]
[source]
#
Call out to HuggingFaceHub’s embedding endpoint for embedding search docs.
Parameters
texts
– The list of texts to embed.
Returns
List of embeddings, one for each text.
embed_query
(
text
:
str
)
→
List
[
float
]
[source]
#
Call out to HuggingFaceHub’s embedding endpoint for embedding query text.
Parameters
text
– The text to embed.
Returns
Embeddings for the text.
pydantic
model
langchain.embeddings.
HuggingFaceInstructEmbeddings
[source]
#
Wrapper around sentence_transformers embedding models.
To use, you should have the
sentence_transformers
and
InstructorEmbedding
python packages installed.
Example
from
langchain.embeddings
import
HuggingFaceInstructEmbeddings
model_name
=
"hkunlp/instructor-large"
model_kwargs
=
{
'device'
:
'cpu'
}
encode_kwargs
=
{
'normalize_embeddings'
:
True
}
hf
=
HuggingFaceInstructEmbeddings
(
model_name
=
model_name
,
model_kwargs
=
model_kwargs
,
encode_kwargs
=
encode_kwargs
)
field
cache_folder
:
Optional
[
str
]
=
None
#
Path to store models.
Can be also set by SENTENCE_TRANSFORMERS_HOME environment variable.
field
embed_instruction
:
str
=
'Represent
the
document
for
retrieval:
'
#
Instruction to use for embedding documents.
field
encode_kwargs
:
Dict
[
str
,
Any
]
[Optional]
#
Key word arguments to pass when calling the
encode
method of the model.
field
model_kwargs
:
Dict
[
str
,
Any
]
[Optional]
#
Key word arguments to pass to the model.
field
model_name
:
str
=
'hkunlp/instructor-large'
#
Model name to use.
field
query_instruction
:
str
=
'Represent
the
question
for
retrieving
supporting
documents:
'
#
Instruction to use for embedding query.
embed_documents
(
texts
:
List
[
str
]
)
→
List
[
List
[
float
]
]
[source]
#
Compute doc embeddings using a HuggingFace instruct model.
Parameters
texts
– The list of texts to embed.
Returns
List of embeddings, one for each text.
embed_query
(
text
:
str
)
→
List
[
float
]
[source]
#
Compute query embeddings using a HuggingFace instruct model.
Parameters
text
– The text to embed.
Returns
Embeddings for the text.
pydantic
model
langchain.embeddings.
LlamaCppEmbeddings
[source]
#
Wrapper around llama.cpp embedding models.
To use, you should have the llama-cpp-python library installed, and provide the
path to the Llama model as a named parameter to the constructor.
Check out:
abetlen/llama-cpp-python
Example
from
langchain.embeddings
import
LlamaCppEmbeddings
llama
=
LlamaCppEmbeddings
(
model_path
=
"/path/to/model.bin"
)
field
f16_kv
:
bool
=
False
#
Use half-precision for key/value cache.
field
logits_all
:
bool
=
False
#
Return logits for all tokens, not just the last token.
field
n_batch
:
Optional
[
int
]
=
8
#
Number of tokens to process in parallel.
Should be a number between 1 and n_ctx.
field
n_ctx
:
int
=
512
#
Token context window.
field
n_gpu_layers
:
Optional
[
int
]
=
None
#
Number of layers to be loaded into gpu memory. Default None.
field
n_parts
:
int
=
-1
#
Number of parts to split the model into.
If -1, the number of parts is automatically determined.
field
n_threads
:
Optional
[
int
]
=
None
#
Number of threads to use. If None, the number
of threads is automatically determined.
field
seed
:
int
=
-1
#
Seed. If -1, a random seed is used.
field
use_mlock
:
bool
=
False
#
Force system to keep model in RAM.
field
vocab_only
:
bool
=
False
#
Only load the vocabulary, no weights.
embed_documents
(
texts
:
List
[
str
]
)
→
List
[
List
[
float
]
]
[source]
#
Embed a list of documents using the Llama model.
Parameters
texts
– The list of texts to embed.
Returns
List of embeddings, one for each text.
embed_query
(
text
:
str
)
→
List
[
float
]
[source]
#
Embed a query using the Llama model.
Parameters
text
– The text to embed.
Returns
Embeddings for the text.
pydantic
model
langchain.embeddings.
MiniMaxEmbeddings
[source]
#
Wrapper around MiniMax’s embedding inference service.
To use, you should have the environment variable
MINIMAX_GROUP_ID
and
MINIMAX_API_KEY
set with your API token, or pass it as a named parameter to
the constructor.
Example
from
langchain.embeddings
import
MiniMaxEmbeddings
embeddings
=
MiniMaxEmbeddings
()
query_text
=
"This is a test query."
query_result
=
embeddings
.
embed_query
(
query_text
)
document_text
=
"This is a test document."
document_result
=
embeddings
.
embed_documents
([
document_text
])
field
embed_type_db
:
str
=
'db'
#
For embed_documents
field
embed_type_query
:
str
=
'query'
#
For embed_query
field
endpoint_url
:
str
=
'https://api.minimax.chat/v1/embeddings'
#
Endpoint URL to use.
field
minimax_api_key
:
Optional
[
str
]
=
None
#
API Key for MiniMax API.
field
minimax_group_id
:
Optional
[
str
]
=
None
#
Group ID for MiniMax API.
field
model
:
str
=
'embo-01'
#
Embeddings model name to use.
embed_documents
(
texts
:
List
[
str
]
)
→
List
[
List
[
float
]
]
[source]
#
Embed documents using a MiniMax embedding endpoint.
Parameters
texts
– The list of texts to embed.
Returns
List of embeddings, one for each text.
embed_query
(
text
:
str
)
→
List
[
float
]
[source]
#
Embed a query using a MiniMax embedding endpoint.
Parameters
text
– The text to embed.
Returns
Embeddings for the text.
pydantic
model
langchain.embeddings.
ModelScopeEmbeddings
[source]
#
Wrapper around modelscope_hub embedding models.
To use, you should have the
modelscope
python package installed.
Example
from
langchain.embeddings
import
ModelScopeEmbeddings
model_id
=
"damo/nlp_corom_sentence-embedding_english-base"
embed
=
ModelScopeEmbeddings
(
model_id
=
model_id
)
field
model_id
:
str
=
'damo/nlp_corom_sentence-embedding_english-base'
#
Model name to use.
embed_documents
(
texts
:
List
[
str
]
)
→
List
[
List
[
float
]
]
[source]
#
Compute doc embeddings using a modelscope embedding model.
Parameters
texts
– The list of texts to embed.
Returns
List of embeddings, one for each text.
embed_query
(
text
:
str
)
→
List
[
float
]
[source]
#
Compute query embeddings using a modelscope embedding model.
Parameters
text
– The text to embed.
Returns
Embeddings for the text.
pydantic
model
langchain.embeddings.
MosaicMLInstructorEmbeddings
[source]
#
Wrapper around MosaicML’s embedding inference service.
To use, you should have the
environment variable
MOSAICML_API_TOKEN
set with your API token, or pass
it as a named parameter to the constructor.
Example
from
langchain.llms
import
MosaicMLInstructorEmbeddings
endpoint_url
=
(
"https://models.hosted-on.mosaicml.hosting/instructor-large/v1/predict"
)
mosaic_llm
=
MosaicMLInstructorEmbeddings
(
endpoint_url
=
endpoint_url
,
mosaicml_api_token
=
"my-api-key"
)
field
embed_instruction
:
str
=
'Represent
the
document
for
retrieval:
'
#
Instruction used to embed documents.
field
endpoint_url
:
str
=
'https://models.hosted-on.mosaicml.hosting/instructor-large/v1/predict'
#
Endpoint URL to use.
field
query_instruction
:
str
=
'Represent
the
question
for
retrieving
supporting
documents:
'
#
Instruction used to embed the query.
field
retry_sleep
:
float
=
1.0
#
How long to try sleeping for if a rate limit is encountered
embed_documents
(
texts
:
List
[
str
]
)
→
List
[
List
[
float
]
]
[source]
#
Embed documents using a MosaicML deployed instructor embedding model.
Parameters
texts
– The list of texts to embed.
Returns
List of embeddings, one for each text.
embed_query
(
text
:
str
)
→
List
[
float
]
[source]
#
Embed a query using a MosaicML deployed instructor embedding model.
Parameters
text
– The text to embed.
Returns
Embeddings for the text.
pydantic
model
langchain.embeddings.
OpenAIEmbeddings
[source]
#
Wrapper around OpenAI embedding models.
To use, you should have the
openai
python package installed, and the
environment variable
OPENAI_API_KEY
set with your API key or pass it
as a named parameter to the constructor.
Example
from
langchain.embeddings
import
OpenAIEmbeddings
openai
=
OpenAIEmbeddings
(
openai_api_key
=
"my-api-key"
)
In order to use the library with Microsoft Azure endpoints, you need to set
the OPENAI_API_TYPE, OPENAI_API_BASE, OPENAI_API_KEY and OPENAI_API_VERSION.
The OPENAI_API_TYPE must be set to ‘azure’ and the others correspond to
the properties of your endpoint.
In addition, the deployment name must be passed as the model parameter.
Example
import
os
os
.
environ
[
"OPENAI_API_TYPE"
]
=
"azure"
os
.
environ
[
"OPENAI_API_BASE"
]
=
"https://<your-endpoint.openai.azure.com/"
os
.
environ
[
"OPENAI_API_KEY"
]
=
"your AzureOpenAI key"
os
.
environ
[
"OPENAI_API_VERSION"
]
=
"2023-03-15-preview"
os
.
environ
[
"OPENAI_PROXY"
]
=
"http://your-corporate-proxy:8080"
from
langchain.embeddings.openai
import
OpenAIEmbeddings
embeddings
=
OpenAIEmbeddings
(
deployment
=
"your-embeddings-deployment-name"
,
model
=
"your-embeddings-model-name"
,
openai_api_base
=
"https://your-endpoint.openai.azure.com/"
,
openai_api_type
=
"azure"
,
)
text
=
"This is a test query."
query_result
=
embeddings
.
embed_query
(
text
)
field
chunk_size
:
int
=
1000
#
Maximum number of texts to embed in each batch
field
max_retries
:
int
=
6
#
Maximum number of retries to make when generating.
field
request_timeout
:
Optional
[
Union
[
float
,
Tuple
[
float
,
float
]
]
]
=
None
#
Timeout in seconds for the OpenAPI request.
embed_documents
(
texts
:
List
[
str
]
,
chunk_size
:
Optional
[
int
]
=
0
)
→
List
[
List
[
float
]
]
[source]
#
Call out to OpenAI’s embedding endpoint for embedding search docs.
Parameters
texts
– The list of texts to embed.
chunk_size
– The chunk size of embeddings. If None, will use the chunk size
specified by the class.
Returns
List of embeddings, one for each text.
embed_query
(
text
:
str
)
→
List
[
float
]
[source]
#
Call out to OpenAI’s embedding endpoint for embedding query text.
Parameters
text
– The text to embed.
Returns
Embedding for the text.
pydantic
model
langchain.embeddings.
SagemakerEndpointEmbeddings
[source]
#
Wrapper around custom Sagemaker Inference Endpoints.
To use, you must supply the endpoint name from your deployed
Sagemaker model & the region where it is deployed.
To authenticate, the AWS client uses the following methods to
automatically load credentials:
https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html
If a specific credential profile should be used, you must pass
the name of the profile from the ~/.aws/credentials file that is to be used.
Make sure the credentials / roles used have the required policies to
access the Sagemaker endpoint.
See:
https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html
field
content_handler
:
langchain.embeddings.sagemaker_endpoint.EmbeddingsContentHandler
[Required]
#
The content handler class that provides an input and
output transform functions to handle formats between LLM
and the endpoint.
field
credentials_profile_name
:
Optional
[
str
]
=
None
#
The name of the profile in the ~/.aws/credentials or ~/.aws/config files, which
has either access keys or role information specified.
If not specified, the default credential profile or, if on an EC2 instance,
credentials from IMDS will be used.
See:
https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html
field
endpoint_kwargs
:
Optional
[
Dict
]
=
None
#
Optional attributes passed to the invoke_endpoint
function. See
`boto3`_
. docs for more info.
.. _boto3: <
https://boto3.amazonaws.com/v1/documentation/api/latest/index.html
>
field
endpoint_name
:
str
=
''
#
The name of the endpoint from the deployed Sagemaker model.
Must be unique within an AWS Region.
field
model_kwargs
:
Optional
[
Dict
]
=
None
#
Key word arguments to pass to the model.
field
region_name
:
str
=
''
#
The aws region where the Sagemaker model is deployed, eg.
us-west-2
.
embed_documents
(
texts
:
List
[
str
]
,
chunk_size
:
int
=
64
)
→
List
[
List
[
float
]
]
[source]
#
Compute doc embeddings using a SageMaker Inference Endpoint.
Parameters
texts
– The list of texts to embed.
chunk_size
– The chunk size defines how many input texts will
be grouped together as request. If None, will use the
chunk size specified by the class.
Returns
List of embeddings, one for each text.
embed_query
(
text
:
str
)
→
List
[
float
]
[source]
#
Compute query embeddings using a SageMaker inference endpoint.
Parameters
text
– The text to embed.
Returns
Embeddings for the text.
pydantic
model
langchain.embeddings.
SelfHostedEmbeddings
[source]
#
Runs custom embedding models on self-hosted remote hardware.
Supported hardware includes auto-launched instances on AWS, GCP, Azure,
and Lambda, as well as servers specified
by IP address and SSH credentials (such as on-prem, or another
cloud like Paperspace, Coreweave, etc.).
To use, you should have the
runhouse
python package installed.
Example using a model load function:
from
langchain.embeddings
import
SelfHostedEmbeddings
from
transformers
import
AutoModelForCausalLM
,
AutoTokenizer
,
pipeline
import
runhouse
as
rh
gpu
=
rh
.
cluster
(
name
=
"rh-a10x"
,
instance_type
=
"A100:1"
)
def
get_pipeline
():
model_id
=
"facebook/bart-large"
tokenizer
=
AutoTokenizer
.
from_pretrained
(
model_id
)
model
=
AutoModelForCausalLM
.
from_pretrained
(
model_id
)
return
pipeline
(
"feature-extraction"
,
model
=
model
,
tokenizer
=
tokenizer
)
embeddings
=
SelfHostedEmbeddings
(
model_load_fn
=
get_pipeline
,
hardware
=
gpu
model_reqs
=
[
"./"
,
"torch"
,
"transformers"
],
)
Example passing in a pipeline path:
from
langchain.embeddings
import
SelfHostedHFEmbeddings
import
runhouse
as
rh
from
transformers
import
pipeline
gpu
=
rh
.
cluster
(
name
=
"rh-a10x"
,
instance_type
=
"A100:1"
)
pipeline
=
pipeline
(
model
=
"bert-base-uncased"
,
task
=
"feature-extraction"
)
rh
.
blob
(
pickle
.
dumps
(
pipeline
),
path
=
"models/pipeline.pkl"
)
.
save
()
.
to
(
gpu
,
path
=
"models"
)
embeddings
=
SelfHostedHFEmbeddings
.
from_pipeline
(
pipeline
=
"models/pipeline.pkl"
,
hardware
=
gpu
,
model_reqs
=
[
"./"
,
"torch"
,
"transformers"
],
)
Validators
raise_deprecation
»
all
fields
set_verbose
»
verbose
field
inference_fn
:
Callable
=
<function
_embed_documents>
#
Inference function to extract the embeddings on the remote hardware.
field
inference_kwargs
:
Any
=
None
#
Any kwargs to pass to the model’s inference function.
embed_documents
(
texts
:
List
[
str
]
)
→
List
[
List
[
float
]
]
[source]
#
Compute doc embeddings using a HuggingFace transformer model.
Parameters
texts
– The list of texts to embed.s
Returns
List of embeddings, one for each text.
embed_query
(
text
:
str
)
→
List
[
float
]
[source]
#
Compute query embeddings using a HuggingFace transformer model.
Parameters
text
– The text to embed.
Returns
Embeddings for the text.
pydantic
model
langchain.embeddings.
SelfHostedHuggingFaceEmbeddings
[source]
#
Runs sentence_transformers embedding models on self-hosted remote hardware.
Supported hardware includes auto-launched instances on AWS, GCP, Azure,
and Lambda, as well as servers specified
by IP address and SSH credentials (such as on-prem, or another cloud
like Paperspace, Coreweave, etc.).
To use, you should have the
runhouse
python package installed.
Example
from
langchain.embeddings
import
SelfHostedHuggingFaceEmbeddings
import
runhouse
as
rh
model_name
=
"sentence-transformers/all-mpnet-base-v2"
gpu
=
rh
.
cluster
(
name
=
"rh-a10x"
,
instance_type
=
"A100:1"
)
hf
=
SelfHostedHuggingFaceEmbeddings
(
model_name
=
model_name
,
hardware
=
gpu
)
Validators
raise_deprecation
»
all
fields
set_verbose
»
verbose
field
hardware
:
Any
=
None
#
Remote hardware to send the inference function to.
field
inference_fn
:
Callable
=
<function
_embed_documents>
#
Inference function to extract the embeddings.
field
load_fn_kwargs
:
Optional
[
dict
]
=
None
#
Key word arguments to pass to the model load function.
field
model_id
:
str
=
'sentence-transformers/all-mpnet-base-v2'
#
Model name to use.
field
model_load_fn
:
Callable
=
<function
load_embedding_model>
#
Function to load the model remotely on the server.
field
model_reqs
:
List
[
str
]
=
['./',
'sentence_transformers',
'torch']
#
Requirements to install on hardware to inference the model.
pydantic
model
langchain.embeddings.
SelfHostedHuggingFaceInstructEmbeddings
[source]
#
Runs InstructorEmbedding embedding models on self-hosted remote hardware.
Supported hardware includes auto-launched instances on AWS, GCP, Azure,
and Lambda, as well as servers specified
by IP address and SSH credentials (such as on-prem, or another
cloud like Paperspace, Coreweave, etc.).
To use, you should have the
runhouse
python package installed.
Example
from
langchain.embeddings
import
SelfHostedHuggingFaceInstructEmbeddings
import
runhouse
as
rh
model_name
=
"hkunlp/instructor-large"
gpu
=
rh
.
cluster
(
name
=
'rh-a10x'
,
instance_type
=
'A100:1'
)
hf
=
SelfHostedHuggingFaceInstructEmbeddings
(
model_name
=
model_name
,
hardware
=
gpu
)
Validators
raise_deprecation
»
all
fields
set_verbose
»
verbose
field
embed_instruction
:
str
=
'Represent
the
document
for
retrieval:
'
#
Instruction to use for embedding documents.
field
model_id
:
str
=
'hkunlp/instructor-large'
#
Model name to use.
field
model_reqs
:
List
[
str
]
=
['./',
'InstructorEmbedding',
'torch']
#
Requirements to install on hardware to inference the model.
field
query_instruction
:
str
=
'Represent
the
question
for
retrieving
supporting
documents:
'
#
Instruction to use for embedding query.
embed_documents
(
texts
:
List
[
str
]
)
→
List
[
List
[
float
]
]
[source]
#
Compute doc embeddings using a HuggingFace instruct model.
Parameters
texts
– The list of texts to embed.
Returns
List of embeddings, one for each text.
embed_query
(
text
:
str
)
→
List
[
float
]
[source]
#
Compute query embeddings using a HuggingFace instruct model.
Parameters
text
– The text to embed.
Returns
Embeddings for the text.
langchain.embeddings.
SentenceTransformerEmbeddings
#
alias of
langchain.embeddings.huggingface.HuggingFaceEmbeddings
pydantic
model
langchain.embeddings.
TensorflowHubEmbeddings
[source]
#
Wrapper around tensorflow_hub embedding models.
To use, you should have the
tensorflow_text
python package installed.
Example
from
langchain.embeddings
import
TensorflowHubEmbeddings
url
=
"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3"
tf
=
TensorflowHubEmbeddings
(
model_url
=
url
)
field
model_url
:
str
=
'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'
#
Model name to use.
embed_documents
(
texts
:
List
[
str
]
)
→
List
[
List
[
float
]
]
[source]
#
Compute doc embeddings using a TensorflowHub embedding model.
Parameters
texts
– The list of texts to embed.
Returns
List of embeddings, one for each text.
embed_query
(
text
:
str
)
→
List
[
float
]
[source]
#
Compute query embeddings using a TensorflowHub embedding model.
Parameters
text
– The text to embed.
Returns
Embeddings for the text.
previous
Chat Models
next
Indexes
By Harrison Chase
© Copyright 2023, Harrison Chase.
Last updated on Jun 09, 2023.